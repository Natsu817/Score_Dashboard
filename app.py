
import os
import streamlit as st
from openai import OpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
import pandas as pd

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ç”Ÿæˆ
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY") # ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ãŸAPIã‚­ãƒ¼ã‚’å–å¾—
client = OpenAI(api_key=OPENAI_API_KEY) # APIã‚­ãƒ¼ã®æŒ‡å®š


# dfã‚’èª­ã¿è¾¼ã‚€
df = pd.read_csv('./Rag/Score_table.csv')
df = df.iloc[:,0:11]

# dbã‚’èª­ã¿è¾¼ã‚€
embedding = OpenAIEmbeddings(model='text-embedding-3-small')
db = FAISS.load_local(
    "./Rag/Score_table",
    embedding,
    allow_dangerous_deserialization=True
)
# retrieverã‚’ä½œæˆ
retriever = db.as_retriever(search_kwargs={"k": 1})

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã²ãªå‹ã‚’ä½œæˆ
prompt_form = ChatPromptTemplate.from_template(
    '''
    ä»¥ä¸‹ã®æ–‡è„ˆã ã‘ã‚’è¸ã¾ãˆã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

    æ–‡è„ˆ: """
    {context}
    """

    è³ªå•:"""
    {question}
    """
    
''')

# ç”ŸæˆAIãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©
model = ChatOpenAI(model_name='gpt-4o-mini', temperature=0)


# Streamlitãƒãƒ£ãƒƒãƒˆUIã®è¡¨ç¤º
st.title("ğŸ” ã‚ãã‚ãã•ã‚“ ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
st.write("ã‚ãã‚ãã•ã‚“ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¸ã‚ˆã†ã“ãã€‚ãã¿ã¯ã€ã©ã‚“ãªãƒ¯ã‚¯ãƒ¯ã‚¯ãŒæ¬²ã—ã„ã‹ãªã€‚")


# ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®è¡¨ç¤ºã‚’ã“ã“ã§è¿½åŠ 
df['å¹´æœˆ'] = df['å¹´æœˆ'].astype(str)
df['é¡ä¼¼ã®ãƒ¯ã‚¯ãƒ¯ã‚¯ã®ç™ºç”Ÿå¹´æœˆ'] = df['é¡ä¼¼ã®ãƒ¯ã‚¯ãƒ¯ã‚¯ã®ç™ºç”Ÿå¹´æœˆ'].astype(str)

st.write("ã‚ãã‚ãã•ã‚“ãŒç­”ãˆã‚‰ã‚Œã‚‹æƒ…å ±ã¯ã“ã‚Œã ã‘ã ã‚ˆ")
st.dataframe(df)

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›å—ä»˜
if query := st.chat_input("ã„ã‚‚ã‚€ã—ã‚«ãƒ³ãƒ‘ãƒ‹ãƒ¼ 2023å¹´ã®æƒ…å ±..."):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’å±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append({"role": "user", "content": query})
    with st.chat_message("user"):
        st.markdown(query)

    # RAGã®ãƒã‚§ãƒ¼ãƒ³å‡¦ç†ã‚’æ§‹ç¯‰
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt_form
        | model
        | StrOutputParser()
    )

    # å›ç­”ç”Ÿæˆ
    response_content = chain.invoke(query)

    # å›ç­”ã‚’å±¥æ­´ã«è¿½åŠ ã—è¡¨ç¤º
    st.session_state.messages.append({"role": "assistant", "content": response_content})
    with st.chat_message("assistant"):
        st.markdown(response_content)


