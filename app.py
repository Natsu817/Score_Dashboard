import os
import streamlit as st
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import OpenAIEmbeddings
import pandas as pd

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# OpenAI APIã‚­ãƒ¼ã‚’å–å¾—ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰ï¼‰
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv('./Rag/Score_table.csv')
df = df.iloc[:, 0:11]

# ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿
embedding = OpenAIEmbeddings(model='text-embedding-3-small')
db = FAISS.load_local(
    "./Rag/Score_table",
    embedding,
    allow_dangerous_deserialization=True
)
retriever = db.as_retriever(search_kwargs={"k": 2})

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
prompt_form = ChatPromptTemplate.from_template(
    '''
    ä»¥ä¸‹ã®æ–‡è„ˆã ã‘ã‚’è¸ã¾ãˆã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

    æ–‡è„ˆ: """
    {context}
    """

    è³ªå•:"""
    {question}
    """
    '''
)

# GPTãƒ¢ãƒ‡ãƒ«ã®è¨­å®šï¼ˆgpt-4o-miniï¼‰
model = ChatOpenAI(model_name='gpt-4o-mini', temperature=0)

# Streamlit UI
st.title("ğŸ” ã‚ãã‚ãã•ã‚“ ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
st.write("ã‚ãã‚ãã•ã‚“ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¸ã‚ˆã†ã“ãã€‚ãã¿ã¯ã€ã©ã‚“ãªãƒ¯ã‚¯ãƒ¯ã‚¯ãŒæ¬²ã—ã„ã‹ãªã€‚")

# ğŸ“Š ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
df['å¹´æœˆ'] = df['å¹´æœˆ'].astype(str)
df['é¡ä¼¼ã®ãƒ¯ã‚¯ãƒ¯ã‚¯ã®ç™ºç”Ÿå¹´æœˆ'] = df['é¡ä¼¼ã®ãƒ¯ã‚¯ãƒ¯ã‚¯ã®ç™ºç”Ÿå¹´æœˆ'].astype(str)

st.write("ã‚ãã‚ãã•ã‚“ãŒç­”ãˆã‚‰ã‚Œã‚‹æƒ…å ±ã¯ã“ã‚Œã ã‘ã ã‚ˆ")
st.dataframe(df)

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
if query := st.chat_input("ã„ã‚‚ã‚€ã—ã‚«ãƒ³ãƒ‘ãƒ‹ãƒ¼ 2023å¹´ã®æƒ…å ±..."):
    st.session_state.messages.append({"role": "user", "content": query})
    with st.chat_message("user"):
        st.markdown(query)

    # ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰ã¨å¿œç­”ç”Ÿæˆ
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt_form
        | model
        | StrOutputParser()
    )
    response_content = chain.invoke(query)

    st.session_state.messages.append({"role": "assistant", "content": response_content})
    with st.chat_message("assistant"):
        st.markdown(response_content)
